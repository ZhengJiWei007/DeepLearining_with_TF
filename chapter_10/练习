1.使用 He 初始化随机选择权重，是否可以将所有权重初始化为相同的值？
    答：不可以,
        (1) 初始化权重应该考虑为随机性，初始化为均值为0，方差为(考虑输入和输出神经元个数)的正态分布。
        (2) 初始化为相同的值，则网络对称,梯度下降不能收敛。
        (3) 权重初始化相同，方向传播算法计算梯度也相同。则更新后权重也相同，这样每一层的所有神经元作用相同。

2.可以将偏置初始化为 0 吗？
    答：可以

3.说出 ELU 激活功能与 ReLU 相比的三个优点。
    答:(1) ELU 无非0导数，避免了RELU死区问题
       (2) ELU 更加平滑，RELU 在 z=0 时斜率从0 突然变成 1，导致梯度下降在z=0 处震荡。
       (3) ELU 可以处理负值，可以缓解梯度消失问题

4.在哪些情况下，您想要使用以下每个激活函数：ELU，leaky ReLU（及其变体），ReLU，tanh，logistic 以及 softmax？
    答：(1) ELU 默认情况下推荐使用ELU 作为激活函数
        (2) 神经网络更快的计算选择 leaky Relu
        (3) Relu 简单易用
        (4) 如果输出你想要得到 -1 -> 1 可以考虑tanh函数，现在很少在隐层使用
        (5) logistic函数一般用在输出层是二元，同时输出概率的二分类
        (6)softmax函数一般用在互拆的多分类，同时输出概率的多分类输出层

5.使用MomentumOptimizer时，如果将momentum超参数设置得太接近 1（例如，0.99999），会发生什么情况？
    答：(1) 将momentum参数设置过大，导致算法在开始阶段下降的很快。但是在逼近最小值时，会在最小值附近震荡。无法达到最优

6.请列举您可以生成稀疏模型的三种方法。
    答：(1) 使用L1正则化
        (2) using TensorFlow’s FTRLOptimizer class.

7.dropout 是否会减慢训练？ 它是否会减慢推断（即预测新的实例）？
    答：(1) 会减慢训练速度，不会减慢预测速度

